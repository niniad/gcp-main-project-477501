#!/usr/bin/env python3
"""
BigQuery ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ†ãƒ¼ãƒ–ãƒ«ã€ã‚«ãƒ©ãƒ æƒ…å ±ã‚’èª¿æŸ»ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

from google.cloud import bigquery
import json
import os

def main():
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
    project_id = "main-project-477501"

    # èªè¨¼æƒ…å ±ã®ãƒ‘ã‚¹ã‚’è¨­å®š
    credentials_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')
    if not credentials_path:
        print("ã‚¨ãƒ©ãƒ¼: GOOGLE_APPLICATION_CREDENTIALSç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    # BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ
    client = bigquery.Client(project=project_id)

    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_id}")
    print("=" * 80)

    # ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
    datasets = list(client.list_datasets())

    if not datasets:
        print("\nâš ï¸  ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        print("\nã€æ¨å¥¨ã€‘ECäº‹æ¥­è€…å‘ã‘ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ–°è¦ä½œæˆã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ã¾ã™ã€‚")
        return

    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(datasets)}")
    print("=" * 80)

    all_data = {}

    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦èª¿æŸ»
    for dataset in datasets:
        dataset_id = dataset.dataset_id
        print(f"\n\nğŸ—‚ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_id}")
        print("-" * 80)

        dataset_ref = client.dataset(dataset_id)
        tables = list(client.list_tables(dataset_id))

        if not tables:
            print("  ãƒ†ãƒ¼ãƒ–ãƒ«ãªã—")
            continue

        print(f"  ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")

        dataset_info = {
            "dataset_id": dataset_id,
            "tables": []
        }

        # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¤ã„ã¦èª¿æŸ»
        for table in tables:
            table_id = table.table_id
            table_ref = dataset_ref.table(table_id)
            table_obj = client.get_table(table_ref)

            print(f"\n  ãƒ†ãƒ¼ãƒ–ãƒ«: {table_id}")
            print(f"    - è¡Œæ•°: {table_obj.num_rows:,}")
            print(f"    - ã‚µã‚¤ã‚º: {table_obj.num_bytes / (1024*1024):.2f} MB")
            print(f"    - ä½œæˆæ—¥: {table_obj.created}")
            print(f"    - æ›´æ–°æ—¥: {table_obj.modified}")

            # ã‚¹ã‚­ãƒ¼ãƒï¼ˆã‚«ãƒ©ãƒ æƒ…å ±ï¼‰ã‚’å–å¾—
            print(f"    - ã‚«ãƒ©ãƒ æ•°: {len(table_obj.schema)}")
            print(f"    - ã‚«ãƒ©ãƒ :")

            columns = []
            for field in table_obj.schema:
                mode = f" ({field.mode})" if field.mode != "NULLABLE" else ""
                description = f" - {field.description}" if field.description else ""
                print(f"      â€¢ {field.name}: {field.field_type}{mode}{description}")

                columns.append({
                    "name": field.name,
                    "type": field.field_type,
                    "mode": field.mode,
                    "description": field.description
                })

            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæœ€åˆã®3è¡Œï¼‰
            query = f"""
                SELECT *
                FROM `{project_id}.{dataset_id}.{table_id}`
                LIMIT 3
            """

            try:
                query_job = client.query(query)
                results = query_job.result()

                print(f"    - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®3è¡Œï¼‰:")
                for i, row in enumerate(results, 1):
                    print(f"      Row {i}: {dict(row)}")
            except Exception as e:
                print(f"    - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

            table_info = {
                "table_id": table_id,
                "num_rows": table_obj.num_rows,
                "size_mb": table_obj.num_bytes / (1024*1024),
                "created": str(table_obj.created),
                "modified": str(table_obj.modified),
                "columns": columns
            }

            dataset_info["tables"].append(table_info)

        all_data[dataset_id] = dataset_info

    # JSONå½¢å¼ã§ã‚‚å‡ºåŠ›
    print("\n\n" + "=" * 80)
    print("ğŸ“„ è©³ç´°æƒ…å ±ï¼ˆJSONå½¢å¼ï¼‰")
    print("=" * 80)
    print(json.dumps(all_data, indent=2, ensure_ascii=False))

    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = "/home/user/gcp-main-project-477501/bigquery_structure.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_data, f, indent=2, ensure_ascii=False)

    print(f"\n\nâœ“ è©³ç´°æƒ…å ±ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
